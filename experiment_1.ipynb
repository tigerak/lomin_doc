{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as tfs\n",
    "torch.manual_seed(42)\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations.augmentations import transforms as AT\n",
    "\n",
    "import imgaug.augmenters as iaa \n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_al(x):\n",
    "    degree = np.random.randint(359)\n",
    "    albumentations_transform = A.Compose([\n",
    "        # A.Resize(256, 256), \n",
    "        # A.Rotate((degree, degree), p=1.0),\n",
    "        A.CenterCrop(128, 128),\n",
    "        \n",
    "        # A.ToGray(p=1),\n",
    "        # A.ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2, \n",
    "        #                 hue=0.2, always_apply=False, p=0.7),\n",
    "        A.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5],\n",
    "            std=[0.5, 0.5, 0.5],\n",
    "        ),\n",
    "        A.pytorch.transforms.ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    start_t = time.time()\n",
    "    x = cv2.imread(x)\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = cv2.resize(x, (256, 256), interpolation=cv2.INTER_LANCZOS4)\n",
    "    augmented = albumentations_transform(image=x) \n",
    "    image = augmented['image']\n",
    "    return image, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_path = './data/test100/01_20210519_072020.png'\n",
    "test_path = './data/test_samples/easyset/3.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, degree = ex_al(exam_path)\n",
    "print(degree)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(tfs.ToPILImage()(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_tf(x):\n",
    "    im_aug = tfs.Compose([\n",
    "            tfs.CenterCrop(128),\n",
    "            # tfs.RandomGrayscale(p=0.8),\n",
    "            # tfs.ColorJitter(brightness=0.2, contrast=0.2, hue=0.2)\n",
    "            ])\n",
    "    \n",
    "    degree = np.random.randint(359) \n",
    "    im_rota = tfs.Compose([\n",
    "        #     tfs.CenterCrop(2048),\n",
    "        #     tfs.Resize(1024),\n",
    "        #     tfs.Resize(512),\n",
    "            # tfs.Resize(256),\n",
    "            # tfs.RandomRotation(degrees=(degree, degree))\n",
    "    ])\n",
    "#     x = x.filter(ImageFilter.DETAIL)\n",
    "    # x = x.resize((256, 256), Image.LANCZOS)\n",
    "    \n",
    "    # x = im_rota(x)\n",
    "    x = im_aug(x)\n",
    "    return x, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam = Image.open(exam_path)\n",
    "score = Image.open(test_path)\n",
    "\n",
    "img, y = ex_tf(exam)\n",
    "print(y)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 경로 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "# mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "# std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "mean = torch.Tensor([0.5]).cuda().half()\n",
    "std = torch.Tensor([0.5]).cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10753\n",
      "1195\n"
     ]
    }
   ],
   "source": [
    "X = glob('./data/train/*.png')\n",
    "\n",
    "DATA_PATH_TRAIN_LIST, DATA_PATH_TEST_LIST = train_test_split(X, \n",
    "                                                            test_size=0.1, \n",
    "                                                            random_state=42)\n",
    "\n",
    "print(len(DATA_PATH_TRAIN_LIST))\n",
    "print(len(DATA_PATH_TEST_LIST))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토치 비젼 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainImageTransform():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.im_aug = tfs.Compose([\n",
    "            # tfs.Resize(256),\n",
    "            tfs.CenterCrop(224),\n",
    "            # tfs.RandomPerspective(), # 실험3. 입체방향 각도의 인식(param부터 다시 얻어와야할듯)\n",
    "            # tfs.RandomRotation(360),\n",
    "            tfs.RandomGrayscale(p=1),\n",
    "            tfs.ColorJitter(brightness=0.3, contrast=0.3, hue=0.3),\n",
    "            tfs.ToTensor(),\n",
    "            tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        degree = np.random.randint(359) \n",
    "        im_rota = tfs.Compose([\n",
    "                tfs.Resize(512),\n",
    "                tfs.RandomRotation(degrees=(degree, degree))\n",
    "        ])\n",
    "        img = im_rota(img)\n",
    "        img = self.im_aug(img)\n",
    "        return img, degree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestImageTransform():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.im_aug = tfs.Compose([\n",
    "            # tfs.Resize(256),\n",
    "            tfs.CenterCrop(224),\n",
    "            # tfs.RandomPerspective() # 실험3. 성능의 차이를 보자 !\n",
    "            tfs.ToTensor(),\n",
    "            tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        degree = np.random.randint(359) \n",
    "        im_rota = tfs.Compose([\n",
    "                tfs.Resize(512),\n",
    "                tfs.RandomRotation(degrees=(degree, degree))\n",
    "        ])\n",
    "        img = im_rota(img)\n",
    "        img = self.im_aug(img)\n",
    "        return img, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Img_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, file_list, transform):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path).convert('RGB') # 픽셀당 추가 알파 채널이 있다고 생각 하므로 채널이 3개가 아닌 4개입니다.\n",
    "        img_transformed, angle = self.transform(img)\n",
    "\n",
    "        return img_transformed, angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = Img_Dataset(file_list=DATA_PATH_TRAIN_LIST,\n",
    "                        transform=TrainImageTransform())\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [9000, 1753])\n",
    "\n",
    "test_dataset = Img_Dataset(file_list=DATA_PATH_TEST_LIST,\n",
    "                        transform=TestImageTransform())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 배치마다 다른 각도 부여 (배치사이즈 64, 32 실험)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, file_list):\n",
    "        self.file_list = file_list\n",
    "\n",
    "        self.degree = np.random.randint(359) \n",
    "        self.im_aug =  A.Compose([\n",
    "            A.Resize(256, 256), \n",
    "            A.Rotate((self.degree, self.degree), p=1.0),\n",
    "            A.CenterCrop(128, 128),\n",
    "            A.ToGray(p=0.6),\n",
    "            A.ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2, \n",
    "                            hue=0.2, always_apply=False, p=0.6\n",
    "                            ),\n",
    "            A.Normalize(\n",
    "                mean=[0.5, 0.5, 0.5],\n",
    "                std=[0.5, 0.5, 0.5]\n",
    "                ),\n",
    "            A.pytorch.transforms.ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_list[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        augmented = self.im_aug(image=image)\n",
    "        image = augmented['image']\n",
    "\n",
    "        return image, self.degree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, file_list):\n",
    "        self.file_list = file_list\n",
    "\n",
    "        self.degree = np.random.randint(359) \n",
    "        self.im_aug =  A.Compose([\n",
    "            A.Resize(256, 256), \n",
    "            A.Rotate((self.degree, self.degree), p=1.0),\n",
    "            A.RandomCrop(128, 128),\n",
    "            A.Normalize(\n",
    "                mean=[0.5, 0.5, 0.5],\n",
    "                std=[0.5, 0.5, 0.5]\n",
    "                ),\n",
    "            A.pytorch.transforms.ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_list[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        augmented = self.im_aug(image=image)\n",
    "        image = augmented['image']\n",
    "        \n",
    "        return image, self.degree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 각 데이터마다 다른 각도 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "\n",
    "        self.im_aug =  A.Compose([\n",
    "            A.Resize(256, 256), \n",
    "            A.CenterCrop(128, 128),\n",
    "            A.ToGray(p=0.6),\n",
    "            A.ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2, \n",
    "                            hue=0.2, always_apply=False, p=0.6\n",
    "                            ),\n",
    "            A.Normalize(\n",
    "                mean=[0.5, 0.5, 0.5],\n",
    "                std=[0.5, 0.5, 0.5]\n",
    "                ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_paths = self.file_paths[idx]\n",
    "\n",
    "        image = cv2.imread(file_paths)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        degree = np.random.randint(359)\n",
    "        al_rotate = A.Compose([\n",
    "                        A.Rotate((degree, degree), p=1.0)\n",
    "        ])\n",
    "        image = al_rotate(image=image)\n",
    "        image = image['image']\n",
    "\n",
    "        augmented = self.im_aug(image=image)\n",
    "        image = augmented['image']\n",
    "\n",
    "        return image, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "\n",
    "        self.im_aug =  A.Compose([\n",
    "            A.Resize(256, 256), \n",
    "            A.RandomCrop(128, 128), ######################## 랜덤 자르기 !!\n",
    "            A.Normalize(\n",
    "                mean=[0.5, 0.5, 0.5],\n",
    "                std=[0.5, 0.5, 0.5]\n",
    "                ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_paths = self.file_paths[idx]\n",
    "\n",
    "        image = cv2.imread(file_paths)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        degree = np.random.randint(359)\n",
    "        al_rotate = A.Compose([\n",
    "                        A.Rotate((degree, degree), p=1.0)\n",
    "        ])\n",
    "        image = al_rotate(image=image)\n",
    "        image = image['image']\n",
    "\n",
    "        augmented = self.im_aug(image=image)\n",
    "        image = augmented['image']\n",
    "\n",
    "        return image, degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# train_val_dataset = TrainDataset(file_paths=DATA_PATH_TRAIN_LIST)\n",
    "\n",
    "# train_dataset, val_dataset = random_split(train_val_dataset, [9000, 1753])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                            batch_size=32, # 실험4. 성능 그래프 확인 요\n",
    "                            shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=32, # 실험4. 성능 그래프 확인 요\n",
    "                            shuffle=True)\n",
    "\n",
    "# test_dataset = TestDataset(file_paths=DATA_PATH_TEST_LIST)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                            batch_size=32,\n",
    "                            shuffle=True)\n",
    "\n",
    "batch_iterator = iter(train_dataloader)\n",
    "images = next(batch_iterator)\n",
    "\n",
    "print(images[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([164,   5, 144, 178, 205,  79,  40, 349, 226, 178, 221,  25, 220, 105,\n",
       "         44, 176,  87, 282, 341,  40, 102,  38, 106, 321,  95, 200, 187,  12,\n",
       "        194, 271, 317, 257])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = {'train': train_dataloader, 'val':val_dataloader, 'test':test_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'VGG1M256': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M', 1024, 1024, 'M'],\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG1M16': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 512, 'M', 1024, 1024, 'M', 1024, 1024, 'M'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_code, in_channels, out_dim, act, use_bn):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        if act == 'relu':\n",
    "            self.act = nn.ReLU()\n",
    "        elif act == 'sigmoid':\n",
    "            self.act = nn.Sigmoid()\n",
    "        elif act == 'tanh':\n",
    "            self.act = nn.TanH()\n",
    "        else:\n",
    "            raise ValueError(\"Not a valid activation function code\")\n",
    "        \n",
    "        self.layers = self._make_layers(model_code, in_channels, use_bn)\n",
    "        self.fcn = nn.Sequential(nn.Conv2d(in_channels=7*7*512,\n",
    "                                           out_channels=4096,\n",
    "                                           kernel_size=1),\n",
    "                                 )\n",
    "        self.classifer = nn.Sequential(nn.Linear(4096, 4096),\n",
    "                                       self.act,\n",
    "                                       nn.Linear(4096, out_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifer(x)\n",
    "        return x\n",
    "        \n",
    "    def _make_layers(self, model_code, in_channels, use_bn):\n",
    "        layers = []\n",
    "        for x in cfg[model_code]:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels=in_channels,\n",
    "                                     out_channels=x,\n",
    "                                     kernel_size=3,\n",
    "                                     stride=1,\n",
    "                                     padding=1)]\n",
    "                if use_bn:\n",
    "                    layers += [nn.BatchNorm2d(x)]\n",
    "                layers += [self.act]\n",
    "                in_channels = x\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim_check():\n",
    "    net = CNN('VGG13', 3, 360, 'relu', True)\n",
    "    x = torch.randn(64, 3, 224, 224)\n",
    "    y = net(x)\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, partition, optimizer, criterion, args):\n",
    "    net.train()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(partition['train'], 0):\n",
    "        optimizer.zero_grad() # [21.01.05 오류 수정] 매 Epoch 마다 .zero_grad()가 실행되는 것을 매 iteration 마다 실행되도록 수정했습니다. \n",
    "\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(partition['train'])\n",
    "    train_acc = 100 * correct / total\n",
    "    return net, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, partition, criterion, args):\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for data in partition['val']:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = net(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(partition['val'])\n",
    "        val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, partition, args):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in partition['test']:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(partition, args):\n",
    "      \n",
    "    net = CNN(model_code = args.model_code,\n",
    "              in_channels = args.in_channels,\n",
    "              out_dim = args.out_dim,\n",
    "              act = args.act,\n",
    "              use_bn = args.use_bn)\n",
    "    net.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if args.optim == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "        \n",
    "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
    "        ts = time.time()\n",
    "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
    "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
    "        te = time.time()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
    "        \n",
    "    test_acc = test(net, partition, args)    \n",
    "    \n",
    "    result = {}\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['train_accs'] = train_accs\n",
    "    result['val_accs'] = val_accs\n",
    "    result['train_acc'] = train_acc\n",
    "    result['val_acc'] = val_acc\n",
    "    result['test_acc'] = test_acc\n",
    "    return vars(args), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "\n",
    "!mkdir results\n",
    "\n",
    "def save_exp_result(setting, result):\n",
    "    exp_name = setting['exp_name']\n",
    "    del setting['epoch']\n",
    "    del setting['test_batch_size']\n",
    "\n",
    "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]\n",
    "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
    "    result.update(setting)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(result, f)\n",
    "\n",
    "    \n",
    "def load_exp_result(exp_name):\n",
    "    dir_path = './results'\n",
    "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]\n",
    "    list_result = []\n",
    "    for filename in filenames:\n",
    "        if exp_name in filename:\n",
    "            with open(join(dir_path, filename), 'r') as infile:\n",
    "                results = json.load(infile)\n",
    "                list_result.append(results)\n",
    "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import argparse\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(var1, var2, df):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(15, 6)\n",
    "    sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "    sns.barplot(x=var1, y='train_acc', hue=var2, data=df, ax=ax[0])\n",
    "    sns.barplot(x=var1, y='val_acc', hue=var2, data=df, ax=ax[1])\n",
    "    sns.barplot(x=var1, y='test_acc', hue=var2, data=df, ax=ax[2])\n",
    "    \n",
    "    ax[0].set_title('Train Accuracy')\n",
    "    ax[1].set_title('Validation Accuracy')\n",
    "    ax[2].set_title('Test Accuracy')\n",
    "\n",
    "    \n",
    "def plot_loss_variation(var1, var2, df, **kwargs):\n",
    "\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_losses = list(row.train_losses)[0]\n",
    "            val_losses = list(row.val_losses)[0]\n",
    "\n",
    "            for epoch, train_loss in enumerate(train_losses):\n",
    "                list_data.append({'type':'train', 'loss':train_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_loss in enumerate(val_losses):\n",
    "                list_data.append({'type':'val', 'loss':val_loss, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'loss', marker='.')\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train loss vs Val loss')\n",
    "    plt.subplots_adjust(top=0.89) # 만약 Title이 그래프랑 겹친다면 top 값을 조정해주면 됩니다! 함수 인자로 받으면 그래프마다 조절할 수 있겠죠?\n",
    "\n",
    "\n",
    "def plot_acc_variation(var1, var2, df, **kwargs):\n",
    "    list_v1 = df[var1].unique()\n",
    "    list_v2 = df[var2].unique()\n",
    "    list_data = []\n",
    "\n",
    "    for value1 in list_v1:\n",
    "        for value2 in list_v2:\n",
    "            row = df.loc[df[var1]==value1]\n",
    "            row = row.loc[df[var2]==value2]\n",
    "\n",
    "            train_accs = list(row.train_accs)[0]\n",
    "            val_accs = list(row.val_accs)[0]\n",
    "            test_acc = list(row.test_acc)[0]\n",
    "\n",
    "            for epoch, train_acc in enumerate(train_accs):\n",
    "                list_data.append({'type':'train', 'Acc':train_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "            for epoch, val_acc in enumerate(val_accs):\n",
    "                list_data.append({'type':'val', 'Acc':val_acc, 'test_acc':test_acc, 'epoch':epoch, var1:value1, var2:value2})\n",
    "\n",
    "    df = pd.DataFrame(list_data)\n",
    "    g = sns.FacetGrid(df, row=var2, col=var1, hue='type', **kwargs)\n",
    "    g = g.map(plt.plot, 'epoch', 'Acc', marker='.')\n",
    "\n",
    "    def show_acc(x, y, metric, **kwargs):\n",
    "        plt.scatter(x, y, alpha=0.3, s=1)\n",
    "        metric = \"Test Acc: {:1.3f}\".format(list(metric.values)[0])\n",
    "        plt.text(0.05, 0.95, metric,  horizontalalignment='left', verticalalignment='center', transform=plt.gca().transAxes, bbox=dict(facecolor='yellow', alpha=0.5, boxstyle=\"round,pad=0.1\"))\n",
    "    g = g.map(show_acc, 'epoch', 'Acc', 'test_acc')\n",
    "\n",
    "    g.add_legend()\n",
    "    g.fig.suptitle('Train Accuracy vs Val Accuracy')\n",
    "    plt.subplots_adjust(top=0.89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Random seed Initialization ====== #\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"exp1_lr_model_code\"\n",
    "\n",
    "# ====== Model ====== #\n",
    "args.model_code = 'VGG13'\n",
    "args.in_channels = 3\n",
    "args.out_dim = 360\n",
    "args.act = 'relu'\n",
    "\n",
    "# ====== Regularization ======= #\n",
    "args.l2 = 0.00001\n",
    "args.use_bn = True\n",
    "\n",
    "# ====== Optimizer & Training ====== #\n",
    "args.optim = 'Adam' #'RMSprop' #SGD, RMSprop, ADAM...\n",
    "args.lr = 0.0001\n",
    "args.epoch = 50\n",
    "\n",
    "args.train_batch_size = 32\n",
    "args.test_batch_size = 32\n",
    "\n",
    "# ====== Experiment Variable ====== #\n",
    "name_var1 = 'lr'\n",
    "name_var2 = 'model_code'\n",
    "list_var1 = [0.0001]\n",
    "list_var2 = ['VGG13']\n",
    "\n",
    "\n",
    "for var1 in list_var1:\n",
    "    for var2 in list_var2:\n",
    "        setattr(args, name_var1, var1)\n",
    "        setattr(args, name_var2, var2)\n",
    "        print(args)\n",
    "                \n",
    "        setting, result = experiment(partition, deepcopy(args))\n",
    "        # save_exp_result(setting, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar1 = 'lr'\n",
    "var2 = 'optim'\n",
    "df = load_exp_result('exp1')\n",
    "\n",
    "plot_acc(var1, var2, df)\n",
    "plot_loss_variation(var1, var2, df, sharey=False) \n",
    "#sharey를 True로 하면 모둔 subplot의 y축의 스케일이 같아집니다.\n",
    "plot_acc_variation(var1, var2, df, margin_titles=True, sharey=True) \n",
    "#margin_titles를 True로 하면 그래프의 가장자리에 var1과 var2 값이 표시되고 False로 하면 각 subplot 위에 표시됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CNN('VGG11', 3, 360, 'relu', True)\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('0001-11-50.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, current_labels, current_preds = test(model, partition, args)    \n",
    "\n",
    "result_test = {}\n",
    "result_test['test_acc'] = test_acc\n",
    "result_test['test_labels'] = current_labels\n",
    "result_test['test_preds'] = current_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mlomin: \u001b[0mweights=['./yolov5/runs/train/doc2007/weights/best.pt'], source=./data/test_samples/pro/, data=yolov5\\data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=True, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=yolov5\\runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  v6.1-21-ge6e36aa torch 1.7.1+cu110 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 367 layers, 46108278 parameters, 0 gradients, 107.8 GFLOPs\n",
      "image 1/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211109_131112737.jpg: 640x480 1 Doc, Done. (0.320s)\n",
      "image 2/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211109_133751394.jpg: 480x640 Done. (0.315s)\n",
      "image 3/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211109_133751394_01.jpg: 640x480 Done. (0.209s)\n",
      "image 4/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211109_133751394_02.jpg: 480x640 1 Doc, Done. (0.179s)\n",
      "image 5/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211109_133751394_03.jpg: 480x640 Done. (0.206s)\n",
      "image 6/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211109_133751394_04.jpg: 480x640 Done. (0.111s)\n",
      "image 7/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211109_133751394_05.jpg: 480x640 1 Doc, Done. (0.035s)\n",
      "image 8/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211109_135137690.jpg: 480x640 2 Docs, Done. (0.033s)\n",
      "image 9/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211109_135137690_01.jpg: 640x480 1 Doc, Done. (0.029s)\n",
      "image 10/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211109_135137690_02.jpg: 640x480 Done. (0.030s)\n",
      "image 11/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211109_135137690_03.jpg: 480x640 1 Doc, Done. (0.029s)\n",
      "image 12/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211109_135137690_04.jpg: 480x640 Done. (0.032s)\n",
      "image 13/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982.jpg: 640x640 1 Doc, Done. (0.051s)\n",
      "image 14/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982_01.jpg: 480x640 1 Doc, Done. (0.102s)\n",
      "image 15/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982_02.jpg: 640x480 1 Doc, Done. (0.027s)\n",
      "image 16/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982_03.jpg: 480x640 Done. (0.282s)\n",
      "image 17/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982_04.jpg: 640x480 1 Doc, Done. (0.305s)\n",
      "image 18/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982_05.jpg: 480x640 Done. (0.252s)\n",
      "image 19/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982_06.jpg: 480x640 Done. (0.204s)\n",
      "image 20/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982_07.jpg: 480x640 1 Doc, Done. (0.182s)\n",
      "image 21/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982_08.jpg: 480x640 1 Doc, Done. (0.192s)\n",
      "image 22/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982_09.jpg: 640x480 1 Doc, Done. (0.160s)\n",
      "image 23/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982_10.jpg: 640x480 2 Docs, Done. (0.047s)\n",
      "image 24/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982_11.jpg: 640x480 1 Doc, Done. (0.031s)\n",
      "image 25/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982_12.jpg: 640x480 1 Doc, Done. (0.031s)\n",
      "image 26/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_125828982_13.jpg: 640x480 1 Doc, Done. (0.025s)\n",
      "image 27/27 D:\\lomin\\data\\test_samples\\pro\\KakaoTalk_20211112_132206556.jpg: 480x640 1 Doc, Done. (0.031s)\n",
      "Speed: 1.2ms pre-process, 127.8ms inference, 4.2ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1myolov5\\runs\\detect\\exp5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from yolov5.lomin import main, parse_opt\n",
    "\n",
    "promp = ['--source', './data/test_samples/pro/', '--weights', './yolov5/runs/train/doc2007/weights/best.pt', '--save-crop']\n",
    "opt = parse_opt(promp)\n",
    "crop_img_list = main(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1077\n",
      "1430\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(crop_img_list))\n",
    "print(len(crop_img_list[0]))\n",
    "print(len(crop_img_list[0][0]))\n",
    "print(len(crop_img_list[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 10, 1: 20, 2: 30, 3: 4}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = { 0 : 10, 1 : 20, 2:30, 3:4}\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(a, key=a.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d39981906b250a5d44fab5ac8e43e664bfe871e4f54ebd8afb9d93d63333f450"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('lomin': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
